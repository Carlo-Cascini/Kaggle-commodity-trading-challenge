{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":94771,"databundleVersionId":13613251,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nMITSUI COMMODITY PREDICTION - KAGGLE SUBMISSION WITH VALIDATION\n================================================================\nHIGH-LEVEL PURPOSE:\nThis code predicts future commodity price movements (spreads between pairs of commodities).\nThink of it like predicting if gold will rise more than silver tomorrow, or if copper \nwill outperform aluminum. We look at 424 different pairs and predict their movements.\n\nThe challenge: We only get ONE row of current prices to make predictions, like trying\nto predict tomorrow's weather with only today's temperature (no history!).\n\nIMPORTANT: Uses rank-based Sharpe ratio - we don't need exact returns, just correct rankings!\n\nEnhanced model with:\n- Memory optimization (70% reduction) - Makes the computer use less memory\n- Feature engineering (+75 production-safe features) - Creates smart measurements\n- Random Forest with optimized parameters - Our prediction machine\n- PROPER TRAIN/TEST SPLIT (0-1826 for training) - Avoids cheating by peeking at test data\n- RANK-BASED SHARPE VALIDATION - Matches Kaggle's exact scoring method\n\"\"\"\n\n# Import libraries - These are like tools in a toolbox\nimport pandas as pd  # Tool for working with tables of data\nimport polars as pl  # Another faster tool for tables (Kaggle uses this)\nimport numpy as np  # Tool for doing math with lots of numbers\nimport random  # Tool for picking random numbers\nimport time  # Tool for measuring how long things take\nimport os, gc  # Tools for computer memory management\nimport warnings  # Tool to hide annoying warning messages\nwarnings.simplefilter('ignore')  # Tell Python to be quiet about warnings\n\n# Import machine learning tools\nfrom sklearn.preprocessing import StandardScaler  # Makes all numbers similar size\nfrom sklearn.ensemble import RandomForestRegressor  # Our prediction machine (like 100 decision trees voting)\n\n# ==============================================================================\n# CONFIGURATION\n# ==============================================================================\n\nclass CFG:\n    \"\"\"Configuration settings - Like the rules of the game\"\"\"\n    path = \"/kaggle/input/mitsui-commodity-prediction-challenge/\"  # Where to find the data files\n    seed = 42  # Magic number that makes random things happen the same way each time\n    targets = [f\"target_{i}\" for i in range(424)]  # Names of 424 things we need to predict\n    solution_null_filler = 0.0  # What number to use when we don't know the answer\n    \n    # PROPER TRAIN/TEST SPLIT MATCHING KAGGLE\n    train_end = 1826      # Training: rows 0-1826 (1827 rows)\n    test_start = 1827     # Test start: row 1827\n    test_end = 1956       # Test end: row 1956 (130 test rows total)\n\n# ==============================================================================\n# MEMORY OPTIMIZATION FUNCTION -\n# ==============================================================================\n\ndef reduce_mem_usage(df):\n    \"\"\"\n    This function makes numbers smaller to save computer memory.\n    Like writing \"1M\" instead of \"1,000,000\" - same meaning, less space!\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2  # How much memory we're using now (in MB)\n    \n    # Look at each column (like each column in a spreadsheet)\n    for col in df.columns:\n        col_type = df[col].dtype  # What type of number is in this column?\n        \n        # If it's a number (not text)\n        if col_type != object:\n            c_min = df[col].min()  # Find the smallest number in this column\n            c_max = df[col].max()  # Find the biggest number in this column\n            \n            # If it's a whole number (like 1, 2, 3, not 1.5)\n            if str(col_type)[:3] == 'int':\n                # Use the smallest box that fits the number\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)  # Tiny box (holds -128 to 127)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)  # Small box (holds -32,768 to 32,767)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)  # Medium box\n            else:  # If it's a decimal number (like 1.5, 2.7)\n                # Use the smallest decimal box that fits\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)  # Tiny decimal box\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)  # Small decimal box\n    \n    end_mem = df.memory_usage().sum() / 1024**2  # How much memory after shrinking\n    print(f'Memory: {start_mem:.2f} MB ‚Üí {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n    return df  # Give back the smaller data\n\n# KAGGLE'S RANK-BASED SHARPE RATIO METRIC \n# ==============================================================================\n\ndef rank_correlation_sharpe_ratio(merged_df: pd.DataFrame) -> float:\n    \"\"\"\n    Calculate Sharpe ratio of rank correlations - the actual competition metric!\n    \n    This is special because it:\n    1. Ranks predictions from 1 to 424 (best to worst)\n    2. Ranks actual values from 1 to 424\n    3. Correlates these ranks (not the actual values!)\n    4. Calculates Sharpe ratio of daily correlations\n    \n    This means we only need to get the ORDER right, not exact values!\n    \"\"\"\n    # Find prediction and target columns\n    prediction_cols = [col for col in merged_df.columns if col.startswith('prediction_')]\n    target_cols = [col for col in merged_df.columns if col.startswith('target_')]\n\n    def _compute_rank_correlation(row):\n        \"\"\"Calculate rank correlation for one day\"\"\"\n        # Find targets that aren't missing (NaN)\n        non_null_targets = [col for col in target_cols if not pd.isnull(row[col])]\n        # Find matching predictions for those targets\n        matching_predictions = [col for col in prediction_cols if col.replace('prediction', 'target') in non_null_targets]\n        \n        # If no valid targets, return 0\n        if not non_null_targets:\n            return 0\n        \n        # If all values are the same (no variance), return 0\n        if row[non_null_targets].std(ddof=0) == 0 or row[matching_predictions].std(ddof=0) == 0:\n            return 0\n        \n        # Calculate correlation between RANKS (not values!)\n        return np.corrcoef(\n            row[matching_predictions].rank(method='average'),  # Rank predictions 1-424\n            row[non_null_targets].rank(method='average')       # Rank actuals 1-424\n        )[0, 1]  # Get correlation coefficient\n\n    # Calculate rank correlation for each day\n    daily_rank_corrs = merged_df.apply(_compute_rank_correlation, axis=1)\n    \n    # Calculate Sharpe ratio (average correlation / standard deviation)\n    std_dev = daily_rank_corrs.std(ddof=0)\n    if std_dev == 0:\n        return 0\n    \n    sharpe_ratio = daily_rank_corrs.mean() / std_dev\n    return float(sharpe_ratio)\n\n# PREPROCESSING FUNCTION \n\ndef preprocess_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Fixes GOLD price columns that sometimes have text instead of numbers.\n    Like fixing \"1,000\" to be 1000.\n    \"\"\"\n    df = df.copy()  # Make a copy so we don't mess up the original\n    # List of GOLD columns that might be broken\n    columns = [\n        \"US_Stock_GOLD_adj_open\",  # Gold's opening price\n        \"US_Stock_GOLD_adj_high\",  # Gold's highest price of the day\n        \"US_Stock_GOLD_adj_low\",   # Gold's lowest price of the day\n        \"US_Stock_GOLD_adj_close\", # Gold's closing price\n        \"US_Stock_GOLD_adj_volume\" # How much gold was traded\n    ]\n    \n    # Fix each column if it exists and has text instead of numbers\n    for col in columns:\n        if col in df.columns and df[col].dtype == \"object\":  # If it's text, not a number\n            df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to number\n    \n    return df\n\n# FEATURE ENGINEERING FUNCTIONS \n\ndef create_commodity_ratios(df):\n    \"\"\"\n    Creates ratios between related commodities.\n    Like measuring how many silver coins equal one gold coin.\n    \"\"\"\n    # Gold/Silver ratio - How much more valuable is gold than silver?\n    if 'US_Stock_GOLD_adj_close' in df.columns and 'US_Stock_SLV_adj_close' in df.columns:\n        df['gold_silver_ratio'] = df['US_Stock_GOLD_adj_close'] / (df['US_Stock_SLV_adj_close'] + 1e-10)\n    \n    # Copper/Aluminum ratio - Industrial metals comparison\n    if 'LME_CU_Close' in df.columns and 'LME_AL_Close' in df.columns:\n        df['copper_aluminum_ratio'] = df['LME_CU_Close'] / (df['LME_AL_Close'] + 1e-10)\n    \n    # Zinc/Lead ratio - Battery metals comparison\n    if 'LME_ZS_Close' in df.columns and 'LME_PB_Close' in df.columns:\n        df['zinc_lead_ratio'] = df['LME_ZS_Close'] / (df['LME_PB_Close'] + 1e-10)\n    \n    # Nickel/Copper ratio - Electronics metals comparison\n    if 'LME_NI_Close' in df.columns and 'LME_CU_Close' in df.columns:\n        df['nickel_copper_ratio'] = df['LME_NI_Close'] / (df['LME_CU_Close'] + 1e-10)\n    \n    return df\n\ndef create_currency_adjusted_prices(df):\n    \"\"\"\n    Adjusts commodity prices for different currencies.\n    Like converting the price of a toy from dollars to euros or yen.\n    \"\"\"\n    # If we have gold prices in dollars\n    if 'US_Stock_GOLD_adj_close' in df.columns:\n        # Convert gold price to euros (European money)\n        if 'FX_EURUSD' in df.columns:\n            df['gold_in_eur'] = df['US_Stock_GOLD_adj_close'] / (df['FX_EURUSD'] + 1e-10)\n        # Convert gold price to British pounds\n        if 'FX_GBPUSD' in df.columns:\n            df['gold_in_gbp'] = df['US_Stock_GOLD_adj_close'] / (df['FX_GBPUSD'] + 1e-10)\n        # Convert gold price to Japanese yen\n        if 'FX_USDJPY' in df.columns:\n            df['gold_in_jpy'] = df['US_Stock_GOLD_adj_close'] * df['FX_USDJPY']\n    \n    # If we have copper prices\n    if 'LME_CU_Close' in df.columns:\n        # Convert copper to British pounds\n        if 'FX_GBPUSD' in df.columns:\n            df['copper_in_gbp'] = df['LME_CU_Close'] / (df['FX_GBPUSD'] + 1e-10)\n        # Convert copper to euros\n        if 'FX_EURUSD' in df.columns:\n            df['copper_in_eur'] = df['LME_CU_Close'] / (df['FX_EURUSD'] + 1e-10)\n    \n    return df\n\ndef create_intraday_features(df):\n    \"\"\"\n    Creates features from daily high/low/open/close prices.\n    Like measuring how wild the price moved during the day.\n    \"\"\"\n    assets = set()  # Empty list to collect asset names\n    # Find all assets that have closing prices\n    for col in df.columns:\n        if '_Close' in col:\n            asset = col.replace('_Close', '')  # Get the asset name\n            # Check if we also have High and Low prices\n            if f'{asset}_High' in df.columns and f'{asset}_Low' in df.columns:\n                assets.add(asset)\n    \n    # For the first 15 assets (to save time)\n    for asset in list(assets)[:15]:\n        if f'{asset}_High' in df.columns and f'{asset}_Low' in df.columns:\n            # How much did price move today? (as percentage)\n            df[f'{asset}_intraday_range'] = (\n                (df[f'{asset}_High'] - df[f'{asset}_Low']) / (df[f'{asset}_Close'] + 1e-10)\n            )\n            # Where did we close? (0 = at lowest, 1 = at highest)\n            df[f'{asset}_close_position'] = (\n                (df[f'{asset}_Close'] - df[f'{asset}_Low']) / \n                (df[f'{asset}_High'] - df[f'{asset}_Low'] + 1e-10)\n            )\n            # High/Low spread\n            df[f'{asset}_hl_spread'] = df[f'{asset}_High'] / (df[f'{asset}_Low'] + 1e-10) - 1\n            \n            # If we also have opening price\n            if f'{asset}_Open' in df.columns:\n                # Did price go up or down today?\n                df[f'{asset}_intraday_return'] = (\n                    (df[f'{asset}_Close'] - df[f'{asset}_Open']) / (df[f'{asset}_Open'] + 1e-10)\n                )\n                # Was it a green day (1) or red day (0)?\n                df[f'{asset}_bullish_day'] = (df[f'{asset}_Close'] > df[f'{asset}_Open']).astype(int)\n    \n    return df\n\ndef create_market_indices(df):\n    \"\"\"\n    Creates average prices for groups of similar assets.\n    Like calculating the average score of all students in a class.\n    \"\"\"\n    # Find all London Metal Exchange metals\n    lme_cols = [col for col in df.columns if 'LME_' in col and '_Close' in col]\n    if len(lme_cols) > 0:\n        df['lme_metals_mean'] = df[lme_cols].mean(axis=1)  # Average price\n        df['lme_metals_std'] = df[lme_cols].std(axis=1)   # How different are prices?\n    \n    # Find all US stocks\n    us_cols = [col for col in df.columns if 'US_Stock_' in col and 'adj_close' in col]\n    if len(us_cols) > 0:\n        df['us_stocks_mean'] = df[us_cols].mean(axis=1)  # Average stock price\n    \n    # Find all USD currency pairs\n    usd_pairs = [col for col in df.columns if 'FX_' in col and 'USD' in col]\n    if len(usd_pairs) > 0:\n        df['usd_strength_index'] = df[usd_pairs].mean(axis=1)  # Dollar strength\n    \n    return df\n\ndef create_spread_features(df):\n    \"\"\"\n    Creates price differences between related assets.\n    Like measuring the gap between first and second place in a race.\n    \"\"\"\n    # Gold minus Silver price\n    if 'US_Stock_GOLD_adj_close' in df.columns and 'US_Stock_SLV_adj_close' in df.columns:\n        df['gold_silver_spread'] = df['US_Stock_GOLD_adj_close'] - df['US_Stock_SLV_adj_close']\n    \n    # Copper minus Aluminum\n    if 'LME_CU_Close' in df.columns and 'LME_AL_Close' in df.columns:\n        df['copper_aluminum_spread'] = df['LME_CU_Close'] - df['LME_AL_Close']\n    \n    # Gold minus Copper (precious vs industrial)\n    if 'US_Stock_GOLD_adj_close' in df.columns and 'LME_CU_Close' in df.columns:\n        df['gold_copper_spread'] = df['US_Stock_GOLD_adj_close'] - df['LME_CU_Close']\n    \n    return df\n\nEPS = 1e-10\n\n# ---------- utils ----------\ndef _safe_div(a, b):\n    return a / (b + EPS)\n\ndef _pct_change(x, n=1):\n    return x.pct_change(n).replace([np.inf, -np.inf], np.nan)\n\ndef _log(x):\n    return np.log(x + EPS)\n\ndef _rolling_zscore(s, win=20):\n    m = s.rolling(win).mean()\n    sd = s.rolling(win).std()\n    return (s - m) / (sd + EPS)\n\ndef _rolling_beta(y, x, win=60):\n    # beta_t = Cov_t(y,x)/Var_t(x)\n    cov = y.rolling(win).cov(x)\n    var = x.rolling(win).var()\n    return cov / (var + EPS)\n\ndef _hedge_residual(y, x, win=60):\n    beta = _rolling_beta(y, x, win=win)\n    # allow intercept via rolling mean adjustment (y - mean_y) - beta*(x - mean_x)\n    y_c = y - y.rolling(win).mean()\n    x_c = x - x.rolling(win).mean()\n    resid = y_c - beta * x_c\n    return resid, beta\n\ndef _lag_cols(df, cols, max_lag):\n    for c in cols:\n        for L in range(1, max_lag + 1):\n            df[f\"{c}_lag{L}\"] = df[c].shift(L)\n    return df\n\ndef _parse_pair(pair_str):\n    # returns list of tokens (A) or (A,B) if \"A - B\"\n    parts = [p.strip() for p in pair_str.split('-') if p.strip()]\n    return parts\n\ndef _assets_from_target_pairs(tp: pd.DataFrame):\n    assets = set()\n    pairs = []\n    for _, r in tp.iterrows():\n        parts = _parse_pair(r[\"pair\"])\n        if len(parts) == 1:\n            assets.add(parts[0])\n            pairs.append((parts[0], None, int(r[\"lag\"])))\n        elif len(parts) == 2:\n            a,b = parts\n            assets.add(a); assets.add(b)\n            pairs.append((a, b, int(r[\"lag\"])))\n    return sorted(assets), pairs\n\n# ---------- NEW: univariate features for all referenced assets ----------\ndef create_univariate_features(df: pd.DataFrame, asset_cols, ret_windows=(1,5,10,20), vol_windows=(10,20,60), ma_windows=(10,20,60)):\n    for col in asset_cols:\n        s = df[col].astype(float)\n        # log price\n        df[f\"{col}_log\"] = _log(s)\n        # returns / momentum\n        for w in ret_windows:\n            df[f\"{col}_ret_{w}\"] = _pct_change(s, n=w)\n            df[f\"{col}_mom_{w}\"] = df[f\"{col}_ret_{w}\"].rolling(w).sum()\n        # rolling volatility (stdev of daily returns)\n        daily_ret = _pct_change(s, n=1)\n        for w in vol_windows:\n            df[f\"{col}_vol_{w}\"] = daily_ret.rolling(w).std()\n        # moving averages and z-scores\n        for w in ma_windows:\n            ma = s.rolling(w).mean()\n            df[f\"{col}_ma_{w}\"] = ma\n            df[f\"{col}_z_{w}\"] = _rolling_zscore(s, win=w)\n        # drawdown (rolling)\n        for w in (60, 120):\n            roll_max = s.rolling(w).max()\n            df[f\"{col}_dd_{w}\"] = (s / (roll_max + EPS)) - 1.0\n    return df\n\n# ---------- NEW: pairwise features driven by target_pairs.csv ----------\ndef create_pair_features_from_targets(df: pd.DataFrame, target_pairs: pd.DataFrame,\n                                      z_windows=(20,60), corr_windows=(20,60), beta_windows=(60,120),\n                                      add_lags=True):\n    # Collect unique assets and (A,B,lag) specs\n    assets, pairs = _assets_from_target_pairs(target_pairs)\n    # Safety: only keep assets existing in df\n    assets = [a for a in assets if a in df.columns]\n\n    # Generate univariate features on these assets\n    df = create_univariate_features(df, assets)\n\n    # Generate pair features\n    created_cols = []\n    max_lag = 0\n    for a, b, L in pairs:\n        # skip if asset(s) missing\n        if a not in df.columns:\n            continue\n        if b is None:\n            # Single asset target: still create a few lagged versions of core features\n            base = a\n            core = [f\"{base}_log\", f\"{base}_ret_1\", f\"{base}_ret_5\", f\"{base}_z_20\", f\"{base}_z_60\"]\n            created_cols += [c for c in core if c in df.columns]\n            max_lag = max(max_lag, L)\n            continue\n\n        if b not in df.columns:\n            continue\n\n        A = df[a].astype(float)\n        B = df[b].astype(float)\n\n        spread = A - B\n        ratio = _safe_div(A, B)\n        log_spread = _log(A) - _log(B)\n        pct_diff = _safe_div(A - B, B)\n\n        base = f\"{a}__minus__{b}\"\n        df[f\"{base}_spread\"] = spread\n        df[f\"{base}_ratio\"]  = ratio\n        df[f\"{base}_logspread\"] = log_spread\n        df[f\"{base}_pctdiff\"] = pct_diff\n\n        # z-scores of spreads\n        for w in z_windows:\n            df[f\"{base}_spread_z{w}\"] = _rolling_zscore(spread, win=w)\n            df[f\"{base}_logspread_z{w}\"] = _rolling_zscore(log_spread, win=w)\n\n        # rolling correlation A vs B\n        for w in corr_windows:\n            df[f\"{base}_corr_{w}\"] = A.pct_change().rolling(w).corr(B.pct_change())\n\n        # rolling hedge residual: y=A, x=B\n        for w in beta_windows:\n            resid, beta = _hedge_residual(A, B, win=w)\n            df[f\"{base}_beta_{w}\"]  = beta\n            df[f\"{base}_resid_{w}\"] = resid\n            df[f\"{base}_residz_{w}\"] = _rolling_zscore(resid, win=w)\n\n        created_cols += [\n            f\"{base}_spread\", f\"{base}_ratio\", f\"{base}_logspread\", f\"{base}_pctdiff\",\n            *[f\"{base}_spread_z{w}\" for w in z_windows],\n            *[f\"{base}_logspread_z{w}\" for w in z_windows],\n            *[f\"{base}_corr_{w}\" for w in corr_windows],\n            *[f\"{base}_beta_{w}\" for w in beta_windows],\n            *[f\"{base}_resid_{w}\" for w in beta_windows],\n            *[f\"{base}_residz_{w}\" for w in beta_windows],\n        ]\n        max_lag = max(max_lag, L)\n\n    # Add lags up to the maximum requested in the CSV\n    if add_lags and max_lag > 0 and created_cols:\n        df = _lag_cols(df, created_cols, max_lag)\n\n    return df\n\n# ---------- OPTIONAL: richer intraday metrics when OHLC available ----------\ndef create_intraday_extras(df: pd.DataFrame):\n    # Existing features you had + ATR/true range and gap metrics\n    tickers = set()\n    for c in df.columns:\n        if c.endswith(\"_Close\"):\n            asset = c[:-6]\n            if f\"{asset}_High\" in df.columns and f\"{asset}_Low\" in df.columns:\n                tickers.add(asset)\n    for asset in list(tickers)[:40]:  # cap for speed\n        H, L, C = df[f\"{asset}_High\"], df[f\"{asset}_Low\"], df[f\"{asset}_Close\"]\n        # True Range and ATR\n        prev_close = C.shift(1)\n        tr = pd.concat([(H - L).abs(),\n                        (H - prev_close).abs(),\n                        (L - prev_close).abs()], axis=1).max(axis=1)\n        df[f\"{asset}_TR\"] = tr\n        for w in (14, 20):\n            df[f\"{asset}_ATR_{w}\"] = tr.rolling(w).mean()\n        # Gap features\n        if f\"{asset}_Open\" in df.columns:\n            O = df[f\"{asset}_Open\"]\n            df[f\"{asset}_gap_open\"] = _safe_div(O - prev_close, prev_close)\n            df[f\"{asset}_gap_close\"] = _safe_div(C - prev_close, prev_close)\n    return df\n\n# ---------- UPDATED master ----------\ndef create_all_features(df: pd.DataFrame, target_pairs_csv: str = None, target_pairs_df: pd.DataFrame = None):\n    \"\"\"\n    Extended master that:\n      1) conserva le tue feature esistenti (ratios, currency adj, intraday, indices, spreads),\n      2) aggiunge feature univariate robuste su tutti gli asset presenti nei target,\n      3) costruisce feature pairwise per OGNI coppia dal CSV dei target (spread/ratio/logspread/z-score/corr/beta/resid),\n      4) applica i lag richiesti nel CSV,\n      5) pulisce e clippa.\n    \"\"\"\n    # ---- include your previous feature builders here if you keep them ----\n    # df = create_commodity_ratios(df)\n    # df = create_currency_adjusted_prices(df)\n    # df = create_intraday_features(df)\n    # df = create_market_indices(df)\n    # df = create_spread_features(df)\n    df = create_intraday_extras(df)\n\n    # ---- load target_pairs ----\n    if target_pairs_df is None:\n        if target_pairs_csv is None:\n            raise ValueError(\"Provide target_pairs_csv or target_pairs_df\")\n        target_pairs_df = pd.read_csv(target_pairs_csv)\n\n    # ---- build pair-driven features ----\n    df = create_pair_features_from_targets(df, target_pairs_df,\n                                           z_windows=(20, 60),\n                                           corr_windows=(20, 60),\n                                           beta_windows=(60, 120),\n                                           add_lags=True)\n\n    # ---- global cleanup ----\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.fillna(0.0, inplace=True)\n    num_cols = df.select_dtypes(include=[np.number]).columns\n    df[num_cols] = df[num_cols].clip(-1e10, 1e10)\n    return df\n\n# ==============================================================================\n# LOAD AND PREPARE TRAINING DATA\n# ==============================================================================\n\nprint(\"=\"*70)\nprint(\"MITSUI COMMODITY PREDICTION - MODEL TRAINING & VALIDATION\")\nprint(\"=\"*70)\nprint(f\"Training: Rows 0-{CFG.train_end} ({CFG.train_end + 1} rows)\")\nprint(f\"Testing:  Rows {CFG.test_start}-{CFG.test_end} ({CFG.test_end - CFG.test_start + 1} rows)\")\nprint(\"=\"*70)\n\nprint(\"\\nüìÇ Loading training data...\")\n# Read the training data file (like opening an Excel file)\ntrain = pd.read_csv(CFG.path + \"train.csv\").sort_values(\"date_id\")\ntrain = reduce_mem_usage(train)  # Make it smaller to save memory\n\n# Read the answers we're trying to predict\ntrain_labels = pd.read_csv(CFG.path + \"train_labels.csv\")\ntrain_labels = reduce_mem_usage(train_labels)\n\nprint(f\"Train shape: {train.shape}, Train labels shape: {train_labels.shape}\")\n\n# Fix any broken GOLD columns\ntrain = preprocess_columns(train)\n\n# Create all our smart features\nprint(\"\\nüîß Creating enhanced features...\")\ntrain_enhanced = create_all_features(train.copy())\ntrain_enhanced = reduce_mem_usage(train_enhanced)\n\n# List all the features we'll use (everything except date_id)\nCFG.features = [c for c in train_enhanced.columns if c not in [\"date_id\"]]\nprint(f\"Total features: {len(CFG.features)} (original + engineered)\")\n\n# ==============================================================================\n# PREPARE TRAINING DATA - PROPER SPLIT!\n# ==============================================================================\n\nprint(f\"\\n\" + \"=\"*50)\nprint(f\"TRAINING MODEL\")\nprint(\"=\"*50)\n\n# ONLY USE ROWS 0-1826 FOR TRAINING\nX_train = train_enhanced.iloc[:CFG.train_end+1][CFG.features].fillna(-1)\nX_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n\n# Get the answers for training period only\ny_train = train_labels.iloc[:CFG.train_end+1][CFG.targets].fillna(CFG.solution_null_filler)\n\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n\n# Scale features (make all numbers similar size)\nprint(\"\\nüîÑ Scaling features...\")\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train).astype(np.float32)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# TRAIN MODEL\n# ==============================================================================\n\nprint(\"\\nüå≥ Training Random Forest model...\")\n# Create our prediction machine (Random Forest = many decision trees voting)\nmodel = RandomForestRegressor(\n    n_estimators=1000,        # Use 100 decision trees\n    max_depth=40,           # Each tree can ask up to 20 questions\n    min_samples_split=5,    # Need at least 5 examples to split\n    min_samples_leaf=2,     # Each final answer needs at least 2 examples\n    max_features='sqrt',    # Each tree looks at sqrt(features) randomly\n    random_state=CFG.seed,  # Makes it reproducible\n    n_jobs=-1,             # Use all computer cores\n    verbose=0              # Don't print progress\n)\n\n# Train the model\nmodel.fit(X_train_scaled, y_train)\nprint(\"‚úÖ Model training complete!\")\n\n# Clean up training data to save memory\ndel X_train, X_train_scaled\ngc.collect()\n\n# ==============================================================================\n# VALIDATION - TEST ON LEADERBOARD ROWS TO PREDICT SCORE!\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VALIDATION ON KAGGLE PUBLIC LEADERBOARD DATA\")\nprint(\"Testing on rows 1827-1956 (130 days)\")\nprint(\"This will predict your expected leaderboard score!\")\nprint(\"=\"*60)\n\n# Prepare test data (the rows Kaggle uses for scoring)\nX_test = train_enhanced.iloc[CFG.test_start:CFG.test_end+1][CFG.features].fillna(-1)\nX_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\ny_test = train_labels.iloc[CFG.test_start:CFG.test_end+1][CFG.targets].fillna(CFG.solution_null_filler)\n\nprint(f\"\\nX_test shape: {X_test.shape}\")\nprint(f\"y_test shape: {y_test.shape}\")\n\n# Scale and predict\nprint(\"\\nüîÆ Making predictions...\")\nX_test_scaled = scaler.transform(X_test).astype(np.float32)\ny_pred = model.predict(X_test_scaled)\n\n# Calculate RANK-BASED Sharpe ratio (Kaggle's metric)\nprint(\"\\nüìä Calculating rank-based Sharpe ratio...\")\n\n# Prepare dataframes for metric calculation\ny_pred_df = pd.DataFrame(y_pred, columns=CFG.targets)\ny_test_df = y_test.reset_index(drop=True)\n\n# Create solution and submission dataframes\nsolution_df = y_test_df.copy()\nsubmission_df = y_pred_df.copy()\n\n# Rename prediction columns\nsubmission_df = submission_df.rename(columns={col: col.replace('target_', 'prediction_') \n                                             for col in submission_df.columns})\n\n# Merge for evaluation\nmerged_df = pd.concat([solution_df, submission_df], axis=1)\n\n# Calculate the rank-based Sharpe ratio\nsharpe_ratio = rank_correlation_sharpe_ratio(merged_df)\n\n# ==============================================================================\n# DISPLAY RESULTS\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULTS - EXPECTED KAGGLE PUBLIC LEADERBOARD SCORE\")\nprint(\"=\"*70)\n\nprint(f\"\\nüìà Performance Metrics:\")\nprint(f\"   {'Rank-Based Sharpe Ratio:':<30} {sharpe_ratio:.6f}\")\nprint(f\"   {'Expected Kaggle Score:':<30} ~{sharpe_ratio:.3f}\")\nprint(f\"\\nüí° Interpretation:\")\nif sharpe_ratio > 0.2:\n    print(f\"   Excellent! This is a strong score for this competition.\")\nelif sharpe_ratio > 0.15:\n    print(f\"   Good score! Room for improvement but competitive.\")\nelif sharpe_ratio > 0.1:\n    print(f\"   Decent baseline. Consider more feature engineering.\")\nelse:\n    print(f\"   Below average. Model may need significant improvements.\")\n\n# Show top features\nprint(f\"\\nüèÜ Top 10 Most Important Features:\")\nfeature_importances = model.feature_importances_\nimportance_df = pd.DataFrame({\n    'feature': CFG.features,\n    'importance': feature_importances\n}).sort_values('importance', ascending=False)\n\nfor idx, row in importance_df.head(10).iterrows():\n    print(f\"   {row['feature'][:40]:<40} {row['importance']:.6f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"NEXT STEPS:\")\nprint(\"- This score should closely match Kaggle's public leaderboard\")\nprint(\"- If it doesn't match, check data preprocessing or split\")\nprint(\"- To improve: try ensemble models, more features, or hyperparameter tuning\")\nprint(\"=\"*70 + \"\\n\")\n\n# Clean up\ndel X_test, X_test_scaled, y_test, y_pred\ngc.collect()\n\n# ==============================================================================\n# PREDICTION FUNCTION FOR KAGGLE SUBMISSION\n# ==============================================================================\n\ndef predict(\n    test: pl.DataFrame,\n    label_lags_1_batch: pl.DataFrame,\n    label_lags_2_batch: pl.DataFrame,\n    label_lags_3_batch: pl.DataFrame,\n    label_lags_4_batch: pl.DataFrame,\n) -> pd.DataFrame:\n    \"\"\"\n    Prediction function that Kaggle calls with new data.\n    Gets one row of current prices and must predict 424 spread returns.\n    \n    Remember: We're scored on RANKING, not exact values!\n    \"\"\"\n    \n    # Convert from Polars to Pandas format\n    test_pd = test.to_pandas()\n    \n    # Fix any broken GOLD columns\n    test_pd = preprocess_columns(test_pd)\n    \n    # Create all our smart features (works with single row!)\n    test_enhanced = create_all_features(test_pd.copy())\n    \n    # Select our features and handle missing values\n    X_test = test_enhanced[CFG.features].fillna(-1)\n    X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n    \n    # Scale the features (using same scaling from training)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Make predictions using our trained model\n    pred = model.predict(X_test_scaled)\n    \n    # Create predictions dataframe\n    predictions = pd.DataFrame(pred, columns=CFG.targets)\n    \n    # Ensure all 424 targets have a prediction\n    for target in CFG.targets:\n        if target not in predictions.columns:\n            predictions[target] = CFG.solution_null_filler\n    \n    return predictions[CFG.targets]  # Return in correct order\n\n# ==============================================================================\n# KAGGLE SUBMISSION SERVER - This runs on Kaggle's computers\n# ==============================================================================\n\nimport kaggle_evaluation.mitsui_inference_server\n\n# Create the server that will call our predict function\ninference_server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n\n# Check if we're running on Kaggle or locally\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    # We're on Kaggle - start the prediction server\n    print(\"üöÄ Starting Kaggle inference server...\")\n    inference_server.serve()\nelse:\n    # We're testing locally - run a test\n    print(\"üß™ Running local gateway test...\")\n    inference_server.run_local_gateway((CFG.path,))\n    print(\"‚úÖ Local test complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}